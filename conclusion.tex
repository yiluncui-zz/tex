\section{Conclusion}

The Naive Bayes model works very well on verb prediction task on NELL ontology information and SVO dataset. The dataset has a large number of verbs and hundreds of millions of SVO documents. We use two feature Naive Bayes on Hadoop to predict verbs within a given label set. 

Instead of Laplace smoothing, we use categorical smoothing to introduce category information of the features into the conditional probability estimators, and into Naive Bayes model. The categorical smoothing significantly improve the precision of this verb prediction Naive Bayes classifier. We also tune smoothing parameters in the categorical smoothing estimator. Though the parameter has effect on the precision, however, the precision is not very sensitive to the parameter within a large range.

We also compare different prior setting ups for the Naive Bayes classifier. Unigram prior and uniform prior are compared in controlled experiment. Though unigram prior seems smarted and is expected to have a better precision as a guessing, it generally doesn't has as good results as uniform prior. We are still not able to interpret this observation.

