\section{Methods}

\subsection{Naive Bayes}
% mention prior

The problem can be defined as a text classification problem with the verbs as labels and the subject and object as features. For each SVO document, the number of features is fixed as two. According to the Naive Bayes conditional independency assumption, given the subject s and the object o, the Naive Bayes formula to predict verb $v$ is provided as below.

\begin{equation}
	\hat{v} = \arg\max_v P(v|so) = \arg\max_v P(v)P(s|v)P(o|v)
\end{equation}

One thing we notice here is that the subject and the object are not symmetric. "Cat eat fish" and "Fish eat cat" should not be considered equal. Based on this intuition, we have two different subject and 

However, there exist duplicated features with different labels. For example, a "cat" can eat a "fish", and can also like a "fish". As a classifier, Naive Bayes only gives the label with the highest conditional probability. It doesn't produce multi labels results for a given input document. In our experiments, when the system make a prediction that is one of the correct labels, there will be one "true positive" and all other possible correct answers will be marked as "false negative".

\subsection{Scalability}

% left to Yilun

\subsection{Stemming}

The data set for SVO tuples are parsed from sentences and not stemmed. Stemming can help to merge different tuples and reduce both feature set size and label set size. However, considering the database for ontology contains unstemmed words, we can't stem the subject and object words in the data file because that stemming will increase the ontology matching failure rate.

Despite of that, verbs can be stemmed to reduce the possible label set size. Stemmed 
% left to Yilun

\subsection{Smoothing}

Considering the sparsity of language models, Naive Bayes model usually use smoothing methods. A Dirichlet prior is often used. The Dirichlet prior pulls the features to a uniform distribution at a certain strength according to parameter $\alpha$. In many cases, as a simplification, plus one smoothing is used as a simplified Dirichlet prior with $\alpha=1$. We try different settings of $alpha$ parameter. The comparison is shown in the result section.

\subsection{Category Smoothing}

% left to Yilun
Nell has a knowledge base with ontology information for many known entities. Usually more than one categories are provided for nouns. For example, a dog is a mammal, an animal, an everything and a pet. Some of the categories have containing relations. For the "dog" example, we can describe the category relation as figure \ref{fig-dog-category-example}. As is shown in the figure, some categories like "mammal" are very close to the entity thus describe some attributes of the entity while others like "everything" are very faraway and thus not informative. In our observations, one degree parent categories are usually more helpful.

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
%   Machine Learning Conferences (ICML 1993 -- ICML 2008) and
%   International Workshops on Machine Learning (ML 1988 -- ML
%   1992). At the time this figure was produced, the number of
%   accepted papers for ICML 2008 was unknown and instead estimated.}
% \label{fig-dog-category-example}
% \end{center}
% \vskip -0.2in
% \end{figure}

In order to use the category information in the Naive Bayes model, we considered two approaches. One is to add category counts as independent features. In this way we increase the number of features for each document to above 4. However, the problem is also obvious that the new features are strongly dependent on other features. If a "dog" is in the feature set, "mammal" and "pet" must also be in the feature set. This seriously breaks the conditional independency assumption in Naive Bayes model and gives word with more categories more weights than those with fewer categories, which makes the result difficult to interpret.


\subsection{Label Set}

% TODO

\subsection{Evaluation and Metric}

We use NELL knowledges as labeled true data for training and testing. $10\%$ data is held out for test purpose and the other $90\%$ is training set. Cross validation could be useful but we don't use it because of the large computing consumption.

As a single-label prediction problem, we use precision as the metric to evaluate the classifier. According to the definition of precision,

\begin{equation}
	p = \frac{number_{truePositive}}{number_{truePositive} + number_{falsePositive}}
\end{equation}

Besides, different verbs for the same pair of subject and object are considered as different test cases. Since our system always predict one verb for a given subject-object pair, we always lose precision and recall on multi-label cases. Actually, considering it's a single-label prediction on a multi-label dataset, the precision is always equal to the recall.




